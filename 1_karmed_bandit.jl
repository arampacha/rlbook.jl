### A Pluto.jl notebook ###
# v0.14.2

using Markdown
using InteractiveUtils

# This Pluto notebook uses @bind for interactivity. When running this notebook outside of Pluto, the following 'mock version' of @bind gives bound variables a default value (instead of an error).
macro bind(def, element)
    quote
        local el = $(esc(element))
        global $(esc(def)) = Core.applicable(Base.get, el) ? Base.get(el) : missing
        el
    end
end

# â•”â•â•¡ 48d4dd64-a47d-11eb-14d1-6161cbc1413a
begin
    import Pkg
    Pkg.activate(mktempdir())
	
    Pkg.add([
        Pkg.PackageSpec(name="Plots"),
        Pkg.PackageSpec(name="PlutoUI"),
		Pkg.PackageSpec(name="StatsBase"),
    ])
	
    using Statistics, Plots, PlutoUI, LinearAlgebra, StatsBase
end

# â•”â•â•¡ 3f4a1865-6b69-4485-a285-b4ebee56c015
q = randn(10)

# â•”â•â•¡ a26cf864-d5a4-4d9e-91ad-fbfe4e385d87
function get_reward(q, a; Ïƒ=1.)
	return randn() * Ïƒ + q[a]
end

# â•”â•â•¡ 351277e6-6777-417b-96b8-023b4ef5707d
get_reward(q, 1)

# â•”â•â•¡ 15f1567e-c69a-4ff6-8276-114628be3018
function experiment(N=100, t=1000, trial=trial; trial_kwargs...)
	opt_pcts = []
	average_rewards = []
	for i = 1:N
		q = randn(10)
		res = trial(q, t; trial_kwargs...)
		push!(opt_pcts, res.opt_pct)
		push!(average_rewards, res.avg_reward)
	end
	return opt_pcts, average_rewards
end

# â•”â•â•¡ baa33a22-49e9-4469-b707-585a786d8124
@bind show0 CheckBox()

# â•”â•â•¡ c311c9d2-cfd1-41b9-a47c-b5d7620e5937
@bind eps1 Slider(0.:0.01:0.2, show_value=true)

# â•”â•â•¡ e084c765-f5e4-4bbb-b6dc-db72207bb6a1
let 
	if show0
		opt_pcts, average_rewards = experiment(ğ›†=eps1)
		p1 = plot()
		for r = average_rewards
			plot!(p1, r, label=nothing, opacity=0.3)
		end
		plot!(p1, mean(average_rewards), lw=3, label="mean", colour="black")

		p2 = plot()
		for pct = opt_pcts
			plot!(p2, pct, label=nothing, opacity=0.3)
		end
		plot!(p2, mean(opt_pcts), lw=3, label="mean", colour="black")
		plot(p1, p2, layout=(2, 1))
	end
end

# â•”â•â•¡ 7fb0c7d1-cd78-4996-aca9-c16dd7989d2a
@bind show1 CheckBox()

# â•”â•â•¡ 09e5bc8f-ed94-4d16-b722-27c3d6aaf56f
@bind upd1 Button("Rerun")

# â•”â•â•¡ 770c1609-10aa-4720-b2d0-b89e64d18a9c
let 
	if show1
		upd1
		N = 200
		p1, p2 = plot(title="Average reward", legend=:bottomright), plot(title="Optimal action %", legend=:bottomright)
		for eps = [0., .01, .1]
			opt_pcts, average_rewards = experiment(N, ğ›†=eps)
			if eps == 0.
				l = "Îµ = 0 (greedy)"
			else
				l = "Îµ = $eps"
			end
			plot!(p1, mean(average_rewards), lw=2, label=l)
			plot!(p2, mean(opt_pcts), lw=2, label=l)
		end
		plot(p1, p2, layout=(2, 1))
	end
end

# â•”â•â•¡ 81afbb4c-84e2-4c40-9983-78915da37df1
@bind show2 CheckBox()

# â•”â•â•¡ b01c2fe5-a4a8-43ea-9c36-4c5b785ea3a0
@bind show_unbiased CheckBox()

# â•”â•â•¡ 6a2bcf9e-f4d4-4e7e-9686-16fe8055c03d
@bind show_opt CheckBox()

# â•”â•â•¡ 2889dd86-c0fc-4cee-9a2d-6b829cb746e1
let 
	if show_opt
		N = 200
		p1, p2 = plot(title="Average reward", legend=:bottomright), plot(title="Optimal action %", legend=:bottomright)
		for eps = [0., .1], q_init = [5., 0.]
			opt_pcts, average_rewards = experiment(N; ğ›†=eps, q_init=q_init)
			if eps == 0.
				l = "Îµ=0 (greedy), qâ‚=$q_init"
			else
				l = "Îµ=$eps, qâ‚=$q_init"
			end
			plot!(p1, mean(average_rewards), lw=2, label=l)
			plot!(p2, mean(opt_pcts), lw=2, label=l)
		end
		plot(p1, p2, layout=(2, 1))
	end
end

# â•”â•â•¡ 6eaf46db-dd07-4a47-96b1-6a07c632a79a
@bind pa Slider(0.:0.01:1., show_value=true, default=0.1)

# â•”â•â•¡ 01d8ac29-c38e-4641-a6c8-8a05add94890
let
	t = 1:100
	c = 1
	na = accumulate(+, [Int(rand()<pa) for _=t])
	
	plot(sqrt.(log.(t)./(na .+ 1e-5)), xlabel="t", ylabel="potential")
	plot!(twinx(), na, colour="red", label="action count")
end

# â•”â•â•¡ 8b4765e8-e2dd-4d0f-b86e-9d5afb6e77d2
@bind show_ubc CheckBox()

# â•”â•â•¡ 09b803c6-f06d-4813-bade-c0f93321daa3
@bind show_grad CheckBox()

# â•”â•â•¡ 655ee62d-cd27-47c8-b3df-622aa539b98f
function softmax(x::Vector)
	return exp.(x .- maximum(x)) ./ sum(exp.(x .- maximum(x)))
end

# â•”â•â•¡ 92a924e6-8189-40e6-9cdf-3fad210267c8
begin
	function randint(low, high)
		return rand(low:high)
	end
	function randint(high)
		return randint(1, high)
	end
end

# â•”â•â•¡ 93b4b77a-8055-4d09-9024-a81efcb7d7f9
randint(10)

# â•”â•â•¡ d4023ff2-cfa3-49a8-9a1c-5eaa3f9b7a0e
function getindex(collection, idx, default)
	try
		collection[idx]
	catch
		default
	end
end

# â•”â•â•¡ 0b68c4f7-ba82-4e86-b4ad-0d52e8d510a4
md"Îµ"

# â•”â•â•¡ 49f80719-524e-426c-b0f5-b8c8e2463692
md"## Îµ-greedy strategies"

# â•”â•â•¡ 4910f9fb-3a75-40ce-bb1f-e5fcfe3b3da4
md"Show plot"

# â•”â•â•¡ 029983e9-40fc-45a6-b4ef-4af0e5d96327
md"## Exercise 2.5 - non-stationary"

# â•”â•â•¡ eb826b48-6fc2-468e-8bc7-d96861307b71
md"Show non-stationary Q"

# â•”â•â•¡ 34b6a4d6-b292-4dda-9eb6-1c6371773f43
md"Show unbiased EMA"

# â•”â•â•¡ e9831e70-5052-494e-8682-a88a37fbbd61
md"## Optimistic initial q estimates"

# â•”â•â•¡ b2df3274-7981-4fb6-82fa-d58354e02b60
md"Show optimistic initial Q"

# â•”â•â•¡ 4b672fe5-fb95-4d00-b8d0-1621b8d62ec9
md"## Upper confidence bound"

# â•”â•â•¡ f4bbe00c-b033-415c-9c8e-e877270adc8f
md"Action probability"

# â•”â•â•¡ 939cb982-427e-4c4f-91fa-dc60acb97a01
md"Show UBC"

# â•”â•â•¡ f35e791f-9abf-4c70-ba2b-67139332b98f
md"## Gradient bandit"

# â•”â•â•¡ ff3f9ee1-d75e-4a48-890b-6c6203eca260
md"> Note: This is rather slow and needs to be opimised"

# â•”â•â•¡ 8bd817d4-9cce-44ee-8595-567119a4cf43
md"## utils"

# â•”â•â•¡ c2417fcc-016d-4be4-a1f0-b32a85b6a5a1
getindex([1,2,3], -1, 0)

# â•”â•â•¡ ef167d48-dd50-4bb3-9ada-d53ec0b9e3a6
function accumulate_mean!(collection, val)
	last = try
		collection[end]
	catch
		0
	end
	push!(collection, last + (val-last)/(length(collection)+1))
end

# â•”â•â•¡ c08545e2-5218-4489-8975-5c3f86c66345
function trial(q, t=1000; ğ›†=0., q_init=0., walk_std=0.)
	k = length(q)
	a_optimal_pct = []
	avg_reward = []
	q_est = zeros(k) .+ q_init
	a_counts = zeros(k)
	for i = 1:t
		
		a_optimal = argmax(q)
		# if ğ›† == 0.
		# 	a = argmax(q_est)
		if rand() > ğ›†
			a = argmax(q_est)
		else
			a = randint(k)
		end
		accumulate_mean!(a_optimal_pct, Int(a == a_optimal))
		
		r = get_reward(q, a)
		accumulate_mean!(avg_reward, r)
		
		a_counts[a] += 1
		q_est[a] += (r-q_est[a])/a_counts[a]
		
		if walk_std â‰  0.
			q = q + randn(k)*walk_std
		end
	end
	return (q_est=q_est, avg_reward=avg_reward, opt_pct=a_optimal_pct)
end

# â•”â•â•¡ 86e725ba-ff06-4aa4-bd85-0c1b6f30065b
let
	res = trial(q, 1000)
	
	# plot(res.opt_pct, label=nothing)
	res.q_est, q
end

# â•”â•â•¡ 018c6afe-6a60-4aee-a4da-8612955b64d2
function ema_trial(q, t=1000; ğ›†=0., q_init=0., walk_std=0., Î±=0.1, unbiased=false)
	k = length(q)
	a_optimal_pct = []
	avg_reward = []
	similar
	q_est = zeros(k)
	a_counts = zeros(k)
	
	oÌ„ = 0
	
	for i = 1:t
		a_optimal = argmax(q)
		if ğ›† â‰  0. && rand() > ğ›†
			a = argmax(q_est)
		else
			a = randint(k)
		end
		accumulate_mean!(a_optimal_pct, Int(a == a_optimal))
		
		r = get_reward(q, a)
		accumulate_mean!(avg_reward, r)
		
		a_counts[a] += 1
		
		if unbiased
			oÌ„ += Î± * (1 - oÌ„)
			Î² = Î± / oÌ„
			q_est[a] += Î±*(r-q_est[a])
		else
			q_est[a] += Î±*(r-q_est[a])
		end
		if walk_std â‰  0.
			q = q + randn(k)*walk_std
		end
	end
	return (q_est=q_est, avg_reward=avg_reward, opt_pct=a_optimal_pct)
end

# â•”â•â•¡ 0bbdea00-060b-4ce0-a3e2-d3dc5d141649
let
	q_est, opt_pct, avgr = ema_trial(q, 1000)
	plot(opt_pct, label=nothing)
end

# â•”â•â•¡ 731acaab-e910-40bf-915e-7868d742abd1
unbiased_ema_trial = (args...; kwargs...) -> ema_trial(args...; kwargs...,  unbiased=true)

# â•”â•â•¡ 3aabf6b6-f22a-47ee-8729-4a7840c0bebc
let 
	if show2
		show_unbiased
		N = 200
		t = 10000
		p1, p2 = plot(title="Average reward", legend=:bottomright), plot(title="Optimal action %", legend=:bottomright)
		opt_pcts, average_rewards = experiment(N, t, trial; ğ›†=0.1, walk_std=0.01)
		plot!(p1, mean(average_rewards), lw=2, label="sample avg")
		plot!(p2, mean(opt_pcts), lw=2, label="sample average")

		opt_pcts, average_rewards = experiment(N, t, ema_trial; ğ›†=0.1, walk_std=0.01)
		plot!(p1, mean(average_rewards), lw=2, label="ema")
		plot!(p2, mean(opt_pcts), lw=2, label="ema")

		if show_unbiased

			opt_pcts, average_rewards = experiment(N, t, ğ›†=0.1, walk_std=0.01, trial=unbiased_ema_trial)
			plot!(p1, mean(average_rewards), lw=2, label="unbiased ema")
			plot!(p2, mean(opt_pcts), lw=2, label="unbiased ema")
		end

		plot(p1, p2, layout=(2, 1))
	end
end

# â•”â•â•¡ d0e247a0-2c40-474c-b359-4d2fc045e882
function ucb_trial(q, t=1000; ğ›†=0., q_init=0., walk_std=0., c=2.)
	k = length(q)
	a_optimal_pct = []
	avg_reward = []
	q_est = zeros(k) .+ q_init
	a_counts = zeros(k)
	for i = 1:t
		
		a_optimal = argmax(q)
		
		potentials = sqrt.(log(i)./(a_counts .+ 1e-5))
		
		a = argmax(q_est + potentials)
		accumulate_mean!(a_optimal_pct, Int(a == a_optimal))
		
		r = get_reward(q, a)
		accumulate_mean!(avg_reward, r)
		
		a_counts[a] += 1
		q_est[a] += (r-q_est[a])/a_counts[a]
		
		if walk_std â‰  0.
			q = q + randn(k)*walk_std
		end
	end
	return (q_est=q_est, avg_reward=avg_reward, opt_pct=a_optimal_pct)
end

# â•”â•â•¡ 286c4c5d-bac8-4a46-9bbb-f1cbb75c9605
let 
	if show_ubc
		N = 100
		t = 1000
		p1, p2 = plot(title="Average reward", legend=:bottomright), plot(title="Optimal action %", legend=:bottomright)

		opt_pcts, average_rewards = experiment(N, ğ›†=0.1)
		plot!(p1, mean(average_rewards), lw=2, label="Îµ = 0.1")
		plot!(p2, mean(opt_pcts), lw=2, label="Îµ = 0.1")
		
		c = 2.
		opt_pcts, average_rewards = experiment(N, t, ucb_trial; c=c)
		plot!(p1, mean(average_rewards), lw=2, label="UCB, c=$c")
		plot!(p2, mean(opt_pcts), lw=2, label="UCB, c=$c")

		plot(p1, p2, layout=(2, 1))
	end
end

# â•”â•â•¡ aa4d7178-5058-4caa-a5a6-1b6c96ede324
function grad_trial(q, t=1000; q_init=0., walk_std=0., Î±=0.1, baseline=true)
	k = length(q)
	a_optimal_pct = []
	avg_reward = []
	
	q_est = zeros(k) .+ q_init
	a_counts = zeros(k) ./ k
	h = ones(k)
	
	for i = 1:t
		
		a_optimal = argmax(q)
		Ï€ = softmax(h)
		a = sample(1:k, ProbabilityWeights(Ï€, 1))
		accumulate_mean!(a_optimal_pct, Int(a == a_optimal))
		
		r = get_reward(q, a)
		accumulate_mean!(avg_reward, r)
		
		if baseline
			rÌ„ = getindex(avg_reward, :end, 0)
		else
			rÌ„ = 0
		end
		# update action probabilities
		for (idx, val) in enumerate(h)
			if idx == a
				h[idx] += Î± * (r - rÌ„) * (1 - Ï€[idx])
			else
				h[idx] += Î± * (r - rÌ„) * Ï€[idx]
			end
		end
		
		if walk_std â‰  0.
			q = q + randn(k)*walk_std
		end
	end
	return (q_est=nothing, avg_reward=avg_reward, opt_pct=a_optimal_pct)
end

# â•”â•â•¡ 609c08da-2250-4788-ac7d-fe381a5d34b6
let
	res = grad_trial(q, 1000, baseline=false)
	
	plot(res.opt_pct, label=nothing)
end

# â•”â•â•¡ adffebe6-685a-46ab-93dd-5699f754ce99
let 
	if show_grad
		N = 100
		T = 1000
		p1, p2 = plot(title="Average reward", legend=:bottomright), plot(title="Optimal action %", legend=:bottomright)

		Î± = 0.1
		opt_pcts, average_rewards = experiment(N, T, grad_trial; Î±=0.1, q_init=4.)
		plot!(p1, mean(average_rewards), lw=2, label="Î±=$Î±")
		plot!(p2, mean(opt_pcts), lw=2, label="Î±=$Î±")
		
		Î± = 0.1
		opt_pcts, average_rewards = experiment(N, T, grad_trial; Î±=0.1, q_init=4., baseline=false)
		plot!(p1, mean(average_rewards), lw=2, label="Î±=$Î±, w/o baseline")
		plot!(p2, mean(opt_pcts), lw=2, label="Î±=$Î±, w/o baseline")

		plot(p1, p2, layout=(2, 1))
	end
end

# â•”â•â•¡ 5f88d99c-1ef3-4223-82bd-3a29a033db54
let
	a = []
	for i = 1:5
		accumulate_mean!(a,i)
	end
	a
end

# â•”â•â•¡ Cell order:
# â•Ÿâ”€48d4dd64-a47d-11eb-14d1-6161cbc1413a
# â• â•3f4a1865-6b69-4485-a285-b4ebee56c015
# â• â•a26cf864-d5a4-4d9e-91ad-fbfe4e385d87
# â• â•351277e6-6777-417b-96b8-023b4ef5707d
# â•Ÿâ”€c08545e2-5218-4489-8975-5c3f86c66345
# â• â•86e725ba-ff06-4aa4-bd85-0c1b6f30065b
# â• â•15f1567e-c69a-4ff6-8276-114628be3018
# â•Ÿâ”€baa33a22-49e9-4469-b707-585a786d8124
# â•Ÿâ”€0b68c4f7-ba82-4e86-b4ad-0d52e8d510a4
# â•Ÿâ”€c311c9d2-cfd1-41b9-a47c-b5d7620e5937
# â• â•e084c765-f5e4-4bbb-b6dc-db72207bb6a1
# â•Ÿâ”€49f80719-524e-426c-b0f5-b8c8e2463692
# â•Ÿâ”€4910f9fb-3a75-40ce-bb1f-e5fcfe3b3da4
# â•Ÿâ”€7fb0c7d1-cd78-4996-aca9-c16dd7989d2a
# â•Ÿâ”€09e5bc8f-ed94-4d16-b722-27c3d6aaf56f
# â• â•770c1609-10aa-4720-b2d0-b89e64d18a9c
# â•Ÿâ”€029983e9-40fc-45a6-b4ef-4af0e5d96327
# â•Ÿâ”€018c6afe-6a60-4aee-a4da-8612955b64d2
# â• â•0bbdea00-060b-4ce0-a3e2-d3dc5d141649
# â• â•731acaab-e910-40bf-915e-7868d742abd1
# â•Ÿâ”€eb826b48-6fc2-468e-8bc7-d96861307b71
# â•Ÿâ”€81afbb4c-84e2-4c40-9983-78915da37df1
# â•Ÿâ”€34b6a4d6-b292-4dda-9eb6-1c6371773f43
# â•Ÿâ”€b01c2fe5-a4a8-43ea-9c36-4c5b785ea3a0
# â• â•3aabf6b6-f22a-47ee-8729-4a7840c0bebc
# â•Ÿâ”€e9831e70-5052-494e-8682-a88a37fbbd61
# â•Ÿâ”€b2df3274-7981-4fb6-82fa-d58354e02b60
# â•Ÿâ”€6a2bcf9e-f4d4-4e7e-9686-16fe8055c03d
# â• â•2889dd86-c0fc-4cee-9a2d-6b829cb746e1
# â•Ÿâ”€4b672fe5-fb95-4d00-b8d0-1621b8d62ec9
# â•Ÿâ”€f4bbe00c-b033-415c-9c8e-e877270adc8f
# â•Ÿâ”€6eaf46db-dd07-4a47-96b1-6a07c632a79a
# â•Ÿâ”€01d8ac29-c38e-4641-a6c8-8a05add94890
# â• â•d0e247a0-2c40-474c-b359-4d2fc045e882
# â•Ÿâ”€939cb982-427e-4c4f-91fa-dc60acb97a01
# â•Ÿâ”€8b4765e8-e2dd-4d0f-b86e-9d5afb6e77d2
# â• â•286c4c5d-bac8-4a46-9bbb-f1cbb75c9605
# â•Ÿâ”€f35e791f-9abf-4c70-ba2b-67139332b98f
# â•Ÿâ”€ff3f9ee1-d75e-4a48-890b-6c6203eca260
# â• â•aa4d7178-5058-4caa-a5a6-1b6c96ede324
# â• â•609c08da-2250-4788-ac7d-fe381a5d34b6
# â•Ÿâ”€09b803c6-f06d-4813-bade-c0f93321daa3
# â• â•adffebe6-685a-46ab-93dd-5699f754ce99
# â•Ÿâ”€8bd817d4-9cce-44ee-8595-567119a4cf43
# â•Ÿâ”€655ee62d-cd27-47c8-b3df-622aa539b98f
# â•Ÿâ”€92a924e6-8189-40e6-9cdf-3fad210267c8
# â•Ÿâ”€93b4b77a-8055-4d09-9024-a81efcb7d7f9
# â•Ÿâ”€d4023ff2-cfa3-49a8-9a1c-5eaa3f9b7a0e
# â•Ÿâ”€c2417fcc-016d-4be4-a1f0-b32a85b6a5a1
# â•Ÿâ”€ef167d48-dd50-4bb3-9ada-d53ec0b9e3a6
# â•Ÿâ”€5f88d99c-1ef3-4223-82bd-3a29a033db54
